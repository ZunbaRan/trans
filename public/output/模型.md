## 阿里百炼
```python
# 百炼

# sk-c96e03bdd60d4242b72e5cff45a1ec97
# qwen-max-latest

import os
from openai import OpenAI

client = OpenAI(
    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key="sk-xxx",
    api_key=os.getenv("DASHSCOPE_API_KEY"), 
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)
completion = client.chat.completions.create(
    model="qwen-plus", # 此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models
    messages=[
        {'role': 'system', 'content': 'You are a helpful assistant.'},
        {'role': 'user', 'content': '你是谁？'}],
    )
    
print(completion.model_dump_json())
```



## 火山方舟

```json
{
      "title": "huoshan-deepseekV3",
      "apiKey": "877e8151-2569-4337-a3e2-04f6ae9d5157",
      "baseURL": "https://ark.cn-beijing.volces.com/api/v3/",
      "model": "ep-20250211101700-ts2d7",
      "type": "normal"
    },
    {
      "title": "doubao-pro-1.5",
      "apiKey": "877e8151-2569-4337-a3e2-04f6ae9d5157",
      "baseURL": "https://ark.cn-beijing.volces.com/api/v3/",
      "model": "ep-20250201221516-pv4zn",
      "type": "normal"
    },
    {
      "title": "huoshan-DeepSeek-R1-Distill-Qwen",
      "apiKey": "877e8151-2569-4337-a3e2-04f6ae9d5157",
      "baseURL": "https://ark.cn-beijing.volces.com/api/v3/",
      "model": "ep-20250211101748-xzjcl",
      "type": "reasoner"
    },
    {
      "title": "huoshan-DeepSeek-R1",
      "apiKey": "877e8151-2569-4337-a3e2-04f6ae9d5157",
      "baseURL": "https://ark.cn-beijing.volces.com/api/v3/",
      "model": "ep-20250211101729-97nwq",
      "type": "reasoner"    
    },
```

### 推理模型
```python
import os
from openai import OpenAI

client = OpenAI(
    # 从环境变量中读取您的方舟API Key
    api_key=os.environ.get("ARK_API_KEY"), 
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    # 深度推理模型耗费时间会较长，建议您设置一个较长的超时时间，推荐为30分钟
    timeout=1800,
    )
response = client.chat.completions.create(
    # 替换 <YOUR_ENDPOINT_ID> 为您的方舟推理接入点 ID
    model="<YOUR_ENDPOINT_ID>",
    messages=[
        {"role": "user", "content": "我要有研究推理模型与非推理模型区别的课题，怎么体现我的专业性"}
    ]
)
# 当触发深度推理时，打印思维链内容
if hasattr(response.choices[0].message, 'reasoning_content'):
    print(response.choices[0].message.reasoning_content)
print(response.choices[0].message.content)
```


## google 官方

AIzaSyAoUPjAqWzwqKxdHE6K3TdkuxVX0Sgg_Y8

### curl test gemini
```shell
curl \
  -X POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-pro-exp-02-05:generateContent\?key\=AIzaSyAoUPjAqWzwqKxdHE6K3TdkuxVX0Sgg_Y8 \
  -H 'Content-Type: application/json' \
  -d @<(echo '{
  "contents": [
    {
      "role": "user",
      "parts": [
        {
          "text": "hello"
        }
      ]
    }
  ],
  "generationConfig": {
    "temperature": 1,
    "topK": 64,
    "topP": 0.95,
    "maxOutputTokens": 8192,
    "responseMimeType": "text/plain"
  }
}')
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "Hello there! How can I help you today?\n"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 2,
    "candidatesTokenCount": 10,
    "totalTokenCount": 12
  },
  "modelVersion": "gemini-2.0-pro-exp-02-05"
}
```


gemini 官方

模型
- gemini-2.0-flash-thinking-exp-01-21
- gemini-2.0-flash
- gemini-1.5-pro

### gemini 官方 sdk 安装
```shell
npm install @google/generative-ai
```

### 根据纯文本输入生成文本

与内容关联的 role 有两种可能的选项 对话：

- user：提供提示的角色。该值是 sendMessage 次通话。
- model：提供响应的角色。此角色可以在以下情况下使用： 使用现有的 history 调用 startChat()。
- 
```ts

const { GoogleGenerativeAI } = require("@google/generative-ai");

const genAI = new GoogleGenerativeAI("YOUR_API_KEY");
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });

const prompt = "Explain how AI works";

const result = await model.generateContent(prompt);
console.log(result.response.text());

// 多轮对话
const { GoogleGenerativeAI } = require("@google/generative-ai");

// Access your API key as an environment variable (see "Set up your API key" above)
const genAI = new GoogleGenerativeAI(process.env.API_KEY);

async function run() {
  // The Gemini 1.5 models are versatile and work with multi-turn conversations (like chat)
  const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash"});

  const chat = model.startChat({
    history: [
      {
        role: "user",
        parts: [{ text: "Hello, I have 2 dogs in my house." }],
      },
      {
        role: "model",
        parts: [{ text: "Great to meet you. What would you like to know?" }],
      },
    ],
    generationConfig: {
      maxOutputTokens: 100,
    },
  });

  const msg = "How many paws are in my house?";

  const result = await chat.sendMessage(msg);
  const response = await result.response;
  const text = response.text();
  console.log(text);
}

run();
```



## gemni 兼容 openai
OpenAI 兼容性

您可以使用 OpenAI 库（Python 和 TypeScript/JavaScript）以及 REST API 访问 Gemini 模型，只需更新三行代码并使用 Gemini API 密钥即可。如果您尚未使用 OpenAI 库，我们建议您直接调用 Gemini API。
```ts
import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

const response = await openai.chat.completions.create({
    model: "gemini-1.5-flash",
    messages: [
        { role: "system", content: "You are a helpful assistant." },
        {
            role: "user",
            content: "Explain to me how AI works",
        },
    ],
});

console.log(response.choices[0].message);
```

## closeai
gemini-1.5-pro-002	
gemini-1.5-pro-latest

## 手动执行
腾讯元宝
gemini-2.0-flash-exp
gemini-2.0-flash-thinking-exp


## 使用模型
- 火山 
deepseekV3
doubao-pro-1.5
deepseek-r1
deepseek-r1-distill-qwen
- 阿里
qwen-max
- closeai
gemini-1.5-pro
- 待定
gemini-2.0-flash-exp
gemini-2.0-flash-thing-exp

https://nasazumi-gemini-45.deno.dev/